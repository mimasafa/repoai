{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "presidential-header",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <h1><strong>ANALISIS BIG DATA MEDIA SOSIAL</strong></h1>\n",
    "  <h2><strong>KAMPANYE PRESIDEN INDONESIA 2024</strong></h2>\n",
    "  <br>\n",
    "  <h3>Berdasarkan Petunjuk Teknis Satria Data 2024</h3>\n",
    "  <br>\n",
    "  <p><em>Comprehensive Analysis of X (Twitter) Social Media Data</em></p>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Executive Summary\n",
    "\n",
    "Analisis ini menggunakan teknik advanced analytics untuk memahami dinamika kampanye presiden Indonesia 2024 melalui data media sosial X (Twitter). Analisis mencakup:\n",
    "\n",
    "1. **Complex Network Analysis**: Struktur jaringan interaksi dan identifikasi influencer\n",
    "2. **Topic Clustering**: Pengelompokan dan evolusi topik diskusi\n",
    "3. **Polarization Analysis**: Pengukuran polarisasi politik dan deteksi echo chamber\n",
    "4. **Advanced Analytics**: Analisis temporal, geografis, dan deteksi bot\n",
    "\n",
    "**Dataset**: `sampel_data_semifinal_satria_data_2024.xlsx - Sheet1.csv` (50,000 records)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-section",
   "metadata": {},
   "source": [
    "## üì¶ Import Libraries dan Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import-libraries",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from collections import Counter, defaultdict\n",
    "import json\n",
    "\n",
    "# Network Analysis\n",
    "import networkx as nx\n",
    "from community import community_louvain\n",
    "import igraph as ig\n",
    "from pyvis.network import Network\n",
    "\n",
    "# NLP and Text Processing\n",
    "import nltk\n",
    "import spacy\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "from bertopic import BERTopic\n",
    "from transformers import pipeline\n",
    "\n",
    "# Indonesian text processing\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from langdetect import detect, LangDetectError\n",
    "\n",
    "# Utilities\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import cosine\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üìä Analysis started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-loading-section",
   "metadata": {},
   "source": [
    "## üìÇ Data Loading dan Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "def load_and_examine_data(file_path):\n",
    "    \"\"\"\n",
    "    Load dataset and perform initial examination\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"üìä Loading dataset...\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        print(f\"‚úÖ Dataset loaded successfully!\")\n",
    "        print(f\"üìà Dataset shape: {df.shape}\")\n",
    "        print(f\"üíæ Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load the main dataset\n",
    "file_path = 'sampel_data_semifinal_satria_data_2024.xlsx - Sheet1.csv'\n",
    "df = load_and_examine_data(file_path)\n",
    "\n",
    "if df is not None:\n",
    "    # Display basic information\n",
    "    print(\"\\nüìã Dataset Info:\")\n",
    "    print(df.info())\n",
    "    \n",
    "    print(\"\\nüìä First 5 rows:\")\n",
    "    display(df.head())\n",
    "    \n",
    "    print(\"\\nüìà Statistical Summary:\")\n",
    "    display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-preprocessing-section",
   "metadata": {},
   "source": [
    "## üßπ Data Preprocessing dan Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-preprocessing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Comprehensive data preprocessing and cleaning\n",
    "    \"\"\"\n",
    "    print(\"üßπ Starting data preprocessing...\")\n",
    "    \n",
    "    # Create a copy to avoid modifying original data\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Convert created_at to datetime\n",
    "    print(\"üìÖ Converting timestamps...\")\n",
    "    df_clean['created_at'] = pd.to_datetime(df_clean['created_at'], errors='coerce')\n",
    "    \n",
    "    # Extract time features\n",
    "    df_clean['hour'] = df_clean['created_at'].dt.hour\n",
    "    df_clean['day_of_week'] = df_clean['created_at'].dt.dayofweek\n",
    "    df_clean['date'] = df_clean['created_at'].dt.date\n",
    "    \n",
    "    # Clean numeric columns\n",
    "    numeric_cols = ['num_retweets', 'frn_cnt', 'flw_cnt', 'sts_cnt', 'lst_cnt']\n",
    "    for col in numeric_cols:\n",
    "        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce').fillna(0)\n",
    "    \n",
    "    # Clean text content\n",
    "    print(\"üìù Cleaning text content...\")\n",
    "    df_clean['content_original'] = df_clean['content'].copy()\n",
    "    \n",
    "    # Remove duplicates\n",
    "    initial_size = len(df_clean)\n",
    "    df_clean = df_clean.drop_duplicates(subset=['content'], keep='first')\n",
    "    print(f\"üóëÔ∏è Removed {initial_size - len(df_clean)} duplicate tweets\")\n",
    "    \n",
    "    # Filter out empty content\n",
    "    df_clean = df_clean.dropna(subset=['content'])\n",
    "    df_clean = df_clean[df_clean['content'].str.strip() != '']\n",
    "    \n",
    "    print(f\"‚úÖ Preprocessing completed. Final dataset shape: {df_clean.shape}\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Preprocess the data\n",
    "if df is not None:\n",
    "    df_clean = preprocess_data(df)\n",
    "    \n",
    "    # Show missing values\n",
    "    print(\"\\n‚ùì Missing values per column:\")\n",
    "    missing_values = df_clean.isnull().sum()\n",
    "    missing_values = missing_values[missing_values > 0]\n",
    "    if len(missing_values) > 0:\n",
    "        display(missing_values.to_frame('Missing Count'))\n",
    "    else:\n",
    "        print(\"‚úÖ No missing values found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda-section",
   "metadata": {},
   "source": [
    "## üìä Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_eda(df):\n",
    "    \"\"\"\n",
    "    Comprehensive Exploratory Data Analysis\n",
    "    \"\"\"\n",
    "    print(\"üìä Performing Exploratory Data Analysis...\")\n",
    "    \n",
    "    # 1. Tweet Type Distribution\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\n",
    "            'Tweet Code Distribution', 'Content Type Distribution',\n",
    "            'Language Distribution', 'Tweets Over Time'\n",
    "        ),\n",
    "        specs=[[{\"type\": \"pie\"}, {\"type\": \"pie\"}],\n",
    "               [{\"type\": \"pie\"}, {\"type\": \"scatter\"}]]\n",
    "    )\n",
    "    \n",
    "    # Tweet code distribution\n",
    "    tcode_counts = df['tcode'].value_counts()\n",
    "    fig.add_trace(\n",
    "        go.Pie(labels=tcode_counts.index, values=tcode_counts.values, name=\"TCode\"),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Content type distribution\n",
    "    type_counts = df['type'].value_counts()\n",
    "    fig.add_trace(\n",
    "        go.Pie(labels=type_counts.index, values=type_counts.values, name=\"Type\"),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Language distribution\n",
    "    lang_counts = df['lang'].value_counts()\n",
    "    fig.add_trace(\n",
    "        go.Pie(labels=lang_counts.index, values=lang_counts.values, name=\"Language\"),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Tweets over time\n",
    "    daily_tweets = df.groupby('date').size().reset_index(name='count')\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=daily_tweets['date'], y=daily_tweets['count'], \n",
    "                  mode='lines+markers', name=\"Daily Tweets\"),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(height=800, title_text=\"üìä Basic Dataset Overview\")\n",
    "    fig.show()\n",
    "    \n",
    "    # 2. Engagement Analysis\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Retweet distribution\n",
    "    axes[0,0].hist(df['num_retweets'], bins=50, alpha=0.7, edgecolor='black')\n",
    "    axes[0,0].set_title('Distribution of Retweets')\n",
    "    axes[0,0].set_xlabel('Number of Retweets')\n",
    "    axes[0,0].set_ylabel('Frequency')\n",
    "    axes[0,0].set_yscale('log')\n",
    "    \n",
    "    # Followers vs Friends\n",
    "    sample_df = df.sample(n=min(1000, len(df)))  # Sample for performance\n",
    "    axes[0,1].scatter(sample_df['frn_cnt'], sample_df['flw_cnt'], alpha=0.6)\n",
    "    axes[0,1].set_title('Followers vs Friends Count')\n",
    "    axes[0,1].set_xlabel('Friends Count')\n",
    "    axes[0,1].set_ylabel('Followers Count')\n",
    "    axes[0,1].set_xscale('log')\n",
    "    axes[0,1].set_yscale('log')\n",
    "    \n",
    "    # Hourly tweet distribution\n",
    "    hourly_tweets = df['hour'].value_counts().sort_index()\n",
    "    axes[1,0].bar(hourly_tweets.index, hourly_tweets.values)\n",
    "    axes[1,0].set_title('Tweet Distribution by Hour')\n",
    "    axes[1,0].set_xlabel('Hour of Day')\n",
    "    axes[1,0].set_ylabel('Number of Tweets')\n",
    "    \n",
    "    # Day of week distribution\n",
    "    dow_labels = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "    dow_tweets = df['day_of_week'].value_counts().sort_index()\n",
    "    axes[1,1].bar(range(7), [dow_tweets.get(i, 0) for i in range(7)])\n",
    "    axes[1,1].set_title('Tweet Distribution by Day of Week')\n",
    "    axes[1,1].set_xlabel('Day of Week')\n",
    "    axes[1,1].set_ylabel('Number of Tweets')\n",
    "    axes[1,1].set_xticks(range(7))\n",
    "    axes[1,1].set_xticklabels(dow_labels)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Statistical Summary\n",
    "    print(\"\\nüìà Key Statistics:\")\n",
    "    print(f\"üìÖ Date range: {df['created_at'].min()} to {df['created_at'].max()}\")\n",
    "    print(f\"üîÑ Total retweets: {df['num_retweets'].sum():,}\")\n",
    "    print(f\"üìä Average retweets per tweet: {df['num_retweets'].mean():.2f}\")\n",
    "    print(f\"üåç Unique locations: {df['loc'].nunique()}\")\n",
    "    print(f\"üë• Estimated unique users: {len(df)}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Perform EDA\n",
    "if 'df_clean' in globals():\n",
    "    df_analyzed = perform_eda(df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "text-preprocessing-section",
   "metadata": {},
   "source": [
    "## üìù Text Preprocessing untuk NLP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "text-preprocessing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Indonesian text processing tools\n",
    "print(\"üîß Initializing Indonesian text processing tools...\")\n",
    "\n",
    "# Create stemmer and stopword remover\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "stopword_factory = StopWordRemoverFactory()\n",
    "stopword_remover = stopword_factory.create_stop_word_remover()\n",
    "\n",
    "# Additional Indonesian stopwords\n",
    "additional_stopwords = {\n",
    "    'rt', 're', 'https', 'http', 'www', 'com', 'co', 'id', 'org',\n",
    "    'yg', 'dgn', 'utk', 'dlm', 'pd', 'tdk', 'sdh', 'blm', 'krn',\n",
    "    'pak', 'bu', 'mas', 'mba', 'bang', 'kak', 'om', 'tante',\n",
    "    'anies', 'prabowo', 'ganjar', 'jokowi', 'baswedan', 'subianto', 'pranowo'\n",
    "}\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Comprehensive text cleaning for Indonesian tweets\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove RT markers and mentions\n",
    "    text = re.sub(r'\\brt\\b', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    text = stopword_remover.remove(text)\n",
    "    \n",
    "    # Remove additional stopwords\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in additional_stopwords and len(word) > 2]\n",
    "    \n",
    "    # Stemming\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "def extract_candidates_mentions(text):\n",
    "    \"\"\"\n",
    "    Extract mentions of presidential candidates\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    \n",
    "    text = str(text).lower()\n",
    "    candidates = []\n",
    "    \n",
    "    # Anies Baswedan keywords\n",
    "    if any(keyword in text for keyword in ['anies', 'baswedan', 'amin', 'muhaimin']):\n",
    "        candidates.append('anies')\n",
    "    \n",
    "    # Prabowo Subianto keywords\n",
    "    if any(keyword in text for keyword in ['prabowo', 'subianto', 'gibran']):\n",
    "        candidates.append('prabowo')\n",
    "    \n",
    "    # Ganjar Pranowo keywords\n",
    "    if any(keyword in text for keyword in ['ganjar', 'pranowo', 'mahfud']):\n",
    "        candidates.append('ganjar')\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "# Apply text preprocessing\n",
    "if 'df_clean' in globals():\n",
    "    print(\"üìù Preprocessing tweet content...\")\n",
    "    tqdm.pandas(desc=\"Cleaning text\")\n",
    "    \n",
    "    df_clean['content_cleaned'] = df_clean['content'].progress_apply(clean_text)\n",
    "    df_clean['candidates_mentioned'] = df_clean['content'].progress_apply(extract_candidates_mentions)\n",
    "    \n",
    "    # Filter out empty cleaned content\n",
    "    df_clean = df_clean[df_clean['content_cleaned'].str.len() > 10]\n",
    "    \n",
    "    print(f\"‚úÖ Text preprocessing completed. {len(df_clean)} tweets remaining.\")\n",
    "    \n",
    "    # Show sample cleaned text\n",
    "    print(\"\\nüìù Sample cleaned tweets:\")\n",
    "    for i in range(min(3, len(df_clean))):\n",
    "        print(f\"Original: {df_clean.iloc[i]['content'][:100]}...\")\n",
    "        print(f\"Cleaned:  {df_clean.iloc[i]['content_cleaned'][:100]}...\")\n",
    "        print(f\"Candidates: {df_clean.iloc[i]['candidates_mentioned']}\")\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "network-analysis-section",
   "metadata": {},
   "source": [
    "## üï∏Ô∏è Network Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "network-construction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_interaction_network(df):\n",
    "    \"\"\"\n",
    "    Build network graph from user interactions\n",
    "    \"\"\"\n",
    "    print(\"üï∏Ô∏è Building interaction network...\")\n",
    "    \n",
    "    # Create directed graph\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Extract user interactions from content\n",
    "    interactions = []\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing interactions\"):\n",
    "        content = str(row['content'])\n",
    "        tcode = row['tcode']\n",
    "        \n",
    "        # Extract mentions and RTs\n",
    "        mentions = re.findall(r'@(\\w+)', content)\n",
    "        rt_pattern = re.findall(r'RT.*?@(\\w+)', content)\n",
    "        \n",
    "        # Create pseudo user ID (in real scenario, you'd have actual user IDs)\n",
    "        user_id = f\"user_{idx}\"\n",
    "        \n",
    "        # Add user node\n",
    "        G.add_node(user_id, \n",
    "                  followers=row.get('flw_cnt', 0),\n",
    "                  friends=row.get('frn_cnt', 0),\n",
    "                  statuses=row.get('sts_cnt', 0),\n",
    "                  location=row.get('loc', ''),\n",
    "                  candidates=row.get('candidates_mentioned', []))\n",
    "        \n",
    "        # Add edges for mentions\n",
    "        for mention in mentions:\n",
    "            target_user = f\"@{mention}\"\n",
    "            G.add_edge(user_id, target_user, \n",
    "                      interaction_type='mention',\n",
    "                      weight=1,\n",
    "                      timestamp=row['created_at'])\n",
    "        \n",
    "        # Add edges for retweets\n",
    "        for rt_user in rt_pattern:\n",
    "            target_user = f\"@{rt_user}\"\n",
    "            G.add_edge(user_id, target_user,\n",
    "                      interaction_type='retweet', \n",
    "                      weight=2,  # Retweets have higher weight\n",
    "                      timestamp=row['created_at'])\n",
    "    \n",
    "    print(f\"‚úÖ Network built: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "    return G\n",
    "\n",
    "def calculate_centrality_measures(G):\n",
    "    \"\"\"\n",
    "    Calculate various centrality measures\n",
    "    \"\"\"\n",
    "    print(\"üìä Calculating centrality measures...\")\n",
    "    \n",
    "    # Convert to undirected for some measures\n",
    "    G_undirected = G.to_undirected()\n",
    "    \n",
    "    # Calculate centrality measures\n",
    "    centrality_measures = {}\n",
    "    \n",
    "    print(\"  üéØ Degree centrality...\")\n",
    "    centrality_measures['degree'] = nx.degree_centrality(G)\n",
    "    centrality_measures['in_degree'] = nx.in_degree_centrality(G)\n",
    "    centrality_measures['out_degree'] = nx.out_degree_centrality(G)\n",
    "    \n",
    "    print(\"  üåâ Betweenness centrality (sample)...\")\n",
    "    # Sample for performance on large networks\n",
    "    sample_nodes = list(G.nodes())[:min(1000, len(G.nodes()))]\n",
    "    centrality_measures['betweenness'] = nx.betweenness_centrality(G, k=len(sample_nodes))\n",
    "    \n",
    "    print(\"  üìè Closeness centrality (sample)...\")\n",
    "    centrality_measures['closeness'] = nx.closeness_centrality(G_undirected)\n",
    "    \n",
    "    print(\"  ‚≠ê Eigenvector centrality...\")\n",
    "    try:\n",
    "        centrality_measures['eigenvector'] = nx.eigenvector_centrality(G, max_iter=100)\n",
    "    except:\n",
    "        print(\"    ‚ö†Ô∏è Eigenvector centrality failed, using PageRank instead\")\n",
    "        centrality_measures['pagerank'] = nx.pagerank(G)\n",
    "    \n",
    "    return centrality_measures\n",
    "\n",
    "def detect_communities(G):\n",
    "    \"\"\"\n",
    "    Detect communities using Louvain algorithm\n",
    "    \"\"\"\n",
    "    print(\"üë• Detecting communities...\")\n",
    "    \n",
    "    # Convert to undirected for community detection\n",
    "    G_undirected = G.to_undirected()\n",
    "    \n",
    "    # Apply Louvain community detection\n",
    "    communities = community_louvain.best_partition(G_undirected)\n",
    "    \n",
    "    # Calculate modularity\n",
    "    modularity = community_louvain.modularity(communities, G_undirected)\n",
    "    \n",
    "    print(f\"  üìä Found {len(set(communities.values()))} communities\")\n",
    "    print(f\"  üìà Modularity: {modularity:.3f}\")\n",
    "    \n",
    "    return communities, modularity\n",
    "\n",
    "# Build and analyze network\n",
    "if 'df_clean' in globals():\n",
    "    # Use a sample for performance (adjust size based on your needs)\n",
    "    sample_size = min(5000, len(df_clean))\n",
    "    df_sample = df_clean.sample(n=sample_size, random_state=42)\n",
    "    \n",
    "    # Build network\n",
    "    G = build_interaction_network(df_sample)\n",
    "    \n",
    "    # Calculate centrality measures\n",
    "    centrality_measures = calculate_centrality_measures(G)\n",
    "    \n",
    "    # Detect communities\n",
    "    communities, modularity = detect_communities(G)\n",
    "    \n",
    "    # Add community information to nodes\n",
    "    nx.set_node_attributes(G, communities, 'community')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "network-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_network(G, centrality_measures, communities, max_nodes=100):\n",
    "    \"\"\"\n",
    "    Create network visualization\n",
    "    \"\"\"\n",
    "    print(f\"üé® Creating network visualization (top {max_nodes} nodes)...\")\n",
    "    \n",
    "    # Select top nodes by degree centrality\n",
    "    top_nodes = sorted(centrality_measures['degree'].items(), \n",
    "                      key=lambda x: x[1], reverse=True)[:max_nodes]\n",
    "    top_node_ids = [node[0] for node in top_nodes]\n",
    "    \n",
    "    # Create subgraph\n",
    "    G_viz = G.subgraph(top_node_ids).copy()\n",
    "    \n",
    "    # Create layout\n",
    "    pos = nx.spring_layout(G_viz, k=3, iterations=50)\n",
    "    \n",
    "    # Prepare node data\n",
    "    node_trace = go.Scatter(\n",
    "        x=[pos[node][0] for node in G_viz.nodes()],\n",
    "        y=[pos[node][1] for node in G_viz.nodes()],\n",
    "        mode='markers+text',\n",
    "        text=[node[:10] + '...' if len(node) > 10 else node for node in G_viz.nodes()],\n",
    "        textposition=\"middle center\",\n",
    "        hovertemplate='<b>%{text}</b><br>' +\n",
    "                     'Degree: %{marker.size}<br>' +\n",
    "                     'Community: %{marker.color}<extra></extra>',\n",
    "        marker=dict(\n",
    "            size=[centrality_measures['degree'].get(node, 0) * 100 + 10 for node in G_viz.nodes()],\n",
    "            color=[communities.get(node, 0) for node in G_viz.nodes()],\n",
    "            colorscale='Viridis',\n",
    "            line=dict(width=2, color='DarkSlateGrey')\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Prepare edge data\n",
    "    edge_trace = []\n",
    "    for edge in G_viz.edges():\n",
    "        x0, y0 = pos[edge[0]]\n",
    "        x1, y1 = pos[edge[1]]\n",
    "        edge_trace.append(go.Scatter(\n",
    "            x=[x0, x1, None],\n",
    "            y=[y0, y1, None],\n",
    "            mode='lines',\n",
    "            line=dict(width=0.5, color='rgba(125,125,125,0.3)'),\n",
    "            hoverinfo='none'\n",
    "        ))\n",
    "    \n",
    "    # Create figure\n",
    "    fig = go.Figure(data=edge_trace + [node_trace],\n",
    "                   layout=go.Layout(\n",
    "                        title='üï∏Ô∏è Social Media Interaction Network',\n",
    "                        titlefont_size=16,\n",
    "                        showlegend=False,\n",
    "                        hovermode='closest',\n",
    "                        margin=dict(b=20,l=5,r=5,t=40),\n",
    "                        annotations=[ dict(\n",
    "                            text=f\"Network with {G_viz.number_of_nodes()} nodes and {G_viz.number_of_edges()} edges<br>\" +\n",
    "                                 f\"Communities: {len(set(communities.values()))} | Modularity: {modularity:.3f}\",\n",
    "                            showarrow=False,\n",
    "                            xref=\"paper\", yref=\"paper\",\n",
    "                            x=0.005, y=-0.002 ) ],\n",
    "                        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)\n",
    "                   ))\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def analyze_influencers(G, centrality_measures, df, top_n=20):\n",
    "    \"\"\"\n",
    "    Identify and analyze top influencers\n",
    "    \"\"\"\n",
    "    print(f\"‚≠ê Analyzing top {top_n} influencers...\")\n",
    "    \n",
    "    # Create influencer dataframe\n",
    "    influencer_data = []\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        node_data = {\n",
    "            'node_id': node,\n",
    "            'degree_centrality': centrality_measures['degree'].get(node, 0),\n",
    "            'in_degree_centrality': centrality_measures['in_degree'].get(node, 0),\n",
    "            'out_degree_centrality': centrality_measures['out_degree'].get(node, 0),\n",
    "            'betweenness_centrality': centrality_measures['betweenness'].get(node, 0),\n",
    "            'closeness_centrality': centrality_measures['closeness'].get(node, 0),\n",
    "        }\n",
    "        \n",
    "        # Add eigenvector or pagerank\n",
    "        if 'eigenvector' in centrality_measures:\n",
    "            node_data['eigenvector_centrality'] = centrality_measures['eigenvector'].get(node, 0)\n",
    "        else:\n",
    "            node_data['pagerank'] = centrality_measures['pagerank'].get(node, 0)\n",
    "        \n",
    "        # Add node attributes\n",
    "        node_attrs = G.nodes[node]\n",
    "        node_data.update(node_attrs)\n",
    "        \n",
    "        influencer_data.append(node_data)\n",
    "    \n",
    "    influencer_df = pd.DataFrame(influencer_data)\n",
    "    \n",
    "    # Sort by degree centrality\n",
    "    top_influencers = influencer_df.nlargest(top_n, 'degree_centrality')\n",
    "    \n",
    "    print(\"\\nüèÜ Top Influencers by Degree Centrality:\")\n",
    "    display(top_influencers[['node_id', 'degree_centrality', 'in_degree_centrality', \n",
    "                           'betweenness_centrality', 'followers', 'candidates']].head(10))\n",
    "    \n",
    "    return influencer_df, top_influencers\n",
    "\n",
    "# Visualize network and analyze influencers\n",
    "if 'G' in globals():\n",
    "    # Create network visualization\n",
    "    network_fig = visualize_network(G, centrality_measures, communities)\n",
    "    \n",
    "    # Analyze influencers\n",
    "    influencer_df, top_influencers = analyze_influencers(G, centrality_measures, df_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "topic-modeling-section",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è Topic Modeling dan Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "topic-modeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_tfidf_analysis(df, max_features=1000):\n",
    "    \"\"\"\n",
    "    Perform TF-IDF analysis on cleaned text\n",
    "    \"\"\"\n",
    "    print(\"üìä Performing TF-IDF analysis...\")\n",
    "    \n",
    "    # Create TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=max_features,\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=2,\n",
    "        max_df=0.8\n",
    "    )\n",
    "    \n",
    "    # Fit and transform the text\n",
    "    tfidf_matrix = vectorizer.fit_transform(df['content_cleaned'])\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Get top terms by TF-IDF score\n",
    "    mean_scores = np.mean(tfidf_matrix.toarray(), axis=0)\n",
    "    top_indices = mean_scores.argsort()[-50:][::-1]\n",
    "    top_terms = [(feature_names[i], mean_scores[i]) for i in top_indices]\n",
    "    \n",
    "    print(\"\\nüîù Top terms by TF-IDF score:\")\n",
    "    for term, score in top_terms[:20]:\n",
    "        print(f\"  {term}: {score:.4f}\")\n",
    "    \n",
    "    return tfidf_matrix, vectorizer, feature_names, top_terms\n",
    "\n",
    "def perform_lda_topic_modeling(df, n_topics=8, max_features=1000):\n",
    "    \"\"\"\n",
    "    Perform LDA topic modeling\n",
    "    \"\"\"\n",
    "    print(f\"üè∑Ô∏è Performing LDA topic modeling with {n_topics} topics...\")\n",
    "    \n",
    "    # Create count vectorizer for LDA\n",
    "    count_vectorizer = CountVectorizer(\n",
    "        max_features=max_features,\n",
    "        min_df=2,\n",
    "        max_df=0.8,\n",
    "        ngram_range=(1, 2)\n",
    "    )\n",
    "    \n",
    "    # Fit and transform\n",
    "    count_matrix = count_vectorizer.fit_transform(df['content_cleaned'])\n",
    "    feature_names = count_vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Create and fit LDA model\n",
    "    lda_model = LatentDirichletAllocation(\n",
    "        n_components=n_topics,\n",
    "        random_state=42,\n",
    "        max_iter=10,\n",
    "        learning_method='online'\n",
    "    )\n",
    "    \n",
    "    lda_model.fit(count_matrix)\n",
    "    \n",
    "    # Get topic-word distributions\n",
    "    def get_top_words(model, feature_names, n_top_words=10):\n",
    "        topics = []\n",
    "        for topic_idx, topic in enumerate(model.components_):\n",
    "            top_words_idx = topic.argsort()[-n_top_words:][::-1]\n",
    "            top_words = [feature_names[i] for i in top_words_idx]\n",
    "            topics.append({\n",
    "                'topic_id': topic_idx,\n",
    "                'words': top_words,\n",
    "                'weights': [topic[i] for i in top_words_idx]\n",
    "            })\n",
    "        return topics\n",
    "    \n",
    "    topics = get_top_words(lda_model, feature_names)\n",
    "    \n",
    "    # Display topics\n",
    "    print(\"\\nüìã Discovered Topics:\")\n",
    "    for topic in topics:\n",
    "        words_str = ', '.join(topic['words'][:8])\n",
    "        print(f\"  Topic {topic['topic_id']}: {words_str}\")\n",
    "    \n",
    "    # Get document-topic distributions\n",
    "    doc_topic_dist = lda_model.transform(count_matrix)\n",
    "    \n",
    "    # Assign dominant topic to each document\n",
    "    dominant_topics = np.argmax(doc_topic_dist, axis=1)\n",
    "    \n",
    "    return lda_model, topics, doc_topic_dist, dominant_topics, count_vectorizer\n",
    "\n",
    "def analyze_candidate_topics(df, topics, dominant_topics):\n",
    "    \"\"\"\n",
    "    Analyze topics by presidential candidates\n",
    "    \"\"\"\n",
    "    print(\"üó≥Ô∏è Analyzing topics by presidential candidates...\")\n",
    "    \n",
    "    # Add topic assignments to dataframe\n",
    "    df_topics = df.copy()\n",
    "    df_topics['dominant_topic'] = dominant_topics\n",
    "    \n",
    "    # Analyze topics by candidate mentions\n",
    "    candidate_topics = {}\n",
    "    \n",
    "    for candidate in ['anies', 'prabowo', 'ganjar']:\n",
    "        # Filter tweets mentioning this candidate\n",
    "        candidate_tweets = df_topics[df_topics['candidates_mentioned'].apply(\n",
    "            lambda x: candidate in x if isinstance(x, list) else False\n",
    "        )]\n",
    "        \n",
    "        if len(candidate_tweets) > 0:\n",
    "            # Count topics for this candidate\n",
    "            topic_counts = candidate_tweets['dominant_topic'].value_counts()\n",
    "            candidate_topics[candidate] = topic_counts.to_dict()\n",
    "            \n",
    "            print(f\"\\nüë§ {candidate.upper()} - Top topics ({len(candidate_tweets)} tweets):\")\n",
    "            for topic_id, count in topic_counts.head().items():\n",
    "                topic_words = ', '.join(topics[topic_id]['words'][:5])\n",
    "                print(f\"  Topic {topic_id}: {count} tweets - {topic_words}\")\n",
    "    \n",
    "    return candidate_topics, df_topics\n",
    "\n",
    "def create_topic_visualizations(topics, doc_topic_dist, df):\n",
    "    \"\"\"\n",
    "    Create topic modeling visualizations\n",
    "    \"\"\"\n",
    "    print(\"üìä Creating topic visualizations...\")\n",
    "    \n",
    "    # 1. Topic distribution\n",
    "    topic_counts = np.sum(doc_topic_dist, axis=0)\n",
    "    topic_labels = [f\"Topic {i}\\n{', '.join(topics[i]['words'][:3])}\" for i in range(len(topics))]\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\n",
    "            'Topic Distribution', 'Topic Weights Heatmap',\n",
    "            'Topic Evolution Over Time', 'Topic-Word Cloud'\n",
    "        ),\n",
    "        specs=[[{\"type\": \"bar\"}, {\"type\": \"heatmap\"}],\n",
    "               [{\"type\": \"scatter\"}, {\"type\": \"scatter\"}]]\n",
    "    )\n",
    "    \n",
    "    # Topic distribution bar chart\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=list(range(len(topics))), y=topic_counts, \n",
    "               text=topic_labels, textposition='auto',\n",
    "               name=\"Topic Distribution\"),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Topic weights heatmap\n",
    "    topic_word_matrix = np.array([topic['weights'][:10] for topic in topics])\n",
    "    word_labels = [topics[0]['words'][:10]][0]  # Use first topic's words as labels\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Heatmap(z=topic_word_matrix, \n",
    "                  x=word_labels,\n",
    "                  y=[f\"Topic {i}\" for i in range(len(topics))],\n",
    "                  colorscale='Viridis',\n",
    "                  name=\"Topic-Word Weights\"),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Topic evolution over time (if date data available)\n",
    "    if 'created_at' in df.columns:\n",
    "        # Assign topics to original dataframe rows\n",
    "        df_viz = df.copy()\n",
    "        if len(df_viz) == len(doc_topic_dist):\n",
    "            df_viz['dominant_topic'] = np.argmax(doc_topic_dist, axis=1)\n",
    "            \n",
    "            # Group by date and topic\n",
    "            daily_topics = df_viz.groupby(['date', 'dominant_topic']).size().reset_index(name='count')\n",
    "            \n",
    "            # Plot evolution for top 3 topics\n",
    "            top_topics = topic_counts.argsort()[-3:][::-1]\n",
    "            \n",
    "            for topic_id in top_topics:\n",
    "                topic_data = daily_topics[daily_topics['dominant_topic'] == topic_id]\n",
    "                if len(topic_data) > 0:\n",
    "                    fig.add_trace(\n",
    "                        go.Scatter(x=topic_data['date'], y=topic_data['count'],\n",
    "                                 mode='lines+markers', \n",
    "                                 name=f\"Topic {topic_id}\"),\n",
    "                        row=2, col=1\n",
    "                    )\n",
    "    \n",
    "    fig.update_layout(height=800, title_text=\"üè∑Ô∏è Topic Modeling Analysis\")\n",
    "    fig.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Perform topic modeling analysis\n",
    "if 'df_clean' in globals():\n",
    "    # TF-IDF Analysis\n",
    "    tfidf_matrix, tfidf_vectorizer, feature_names, top_terms = perform_tfidf_analysis(df_clean)\n",
    "    \n",
    "    # LDA Topic Modeling\n",
    "    lda_model, topics, doc_topic_dist, dominant_topics, count_vectorizer = perform_lda_topic_modeling(df_clean)\n",
    "    \n",
    "    # Analyze topics by candidates\n",
    "    candidate_topics, df_with_topics = analyze_candidate_topics(df_clean, topics, dominant_topics)\n",
    "    \n",
    "    # Create visualizations\n",
    "    topic_fig = create_topic_visualizations(topics, doc_topic_dist, df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sentiment-analysis-section",
   "metadata": {},
   "source": [
    "## üòäüò° Sentiment Analysis dan Polarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sentiment-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(df):\n",
    "    \"\"\"\n",
    "    Perform sentiment analysis on tweets\n",
    "    \"\"\"\n",
    "    print(\"üòä Performing sentiment analysis...\")\n",
    "    \n",
    "    # Initialize sentiment analyzer\n",
    "    from textblob import TextBlob\n",
    "    \n",
    "    def get_sentiment(text):\n",
    "        \"\"\"Get sentiment polarity and subjectivity\"\"\"\n",
    "        try:\n",
    "            blob = TextBlob(str(text))\n",
    "            return blob.sentiment.polarity, blob.sentiment.subjectivity\n",
    "        except:\n",
    "            return 0.0, 0.0\n",
    "    \n",
    "    def classify_sentiment(polarity):\n",
    "        \"\"\"Classify sentiment into categories\"\"\"\n",
    "        if polarity > 0.1:\n",
    "            return 'positive'\n",
    "        elif polarity < -0.1:\n",
    "            return 'negative'\n",
    "        else:\n",
    "            return 'neutral'\n",
    "    \n",
    "    # Apply sentiment analysis\n",
    "    tqdm.pandas(desc=\"Analyzing sentiment\")\n",
    "    sentiment_results = df['content_cleaned'].progress_apply(get_sentiment)\n",
    "    \n",
    "    # Extract polarity and subjectivity\n",
    "    df['sentiment_polarity'] = [result[0] for result in sentiment_results]\n",
    "    df['sentiment_subjectivity'] = [result[1] for result in sentiment_results]\n",
    "    df['sentiment_category'] = df['sentiment_polarity'].apply(classify_sentiment)\n",
    "    \n",
    "    # Display sentiment distribution\n",
    "    sentiment_dist = df['sentiment_category'].value_counts()\n",
    "    print(f\"\\nüìä Sentiment Distribution:\")\n",
    "    for category, count in sentiment_dist.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"  {category.title()}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def analyze_candidate_sentiment(df):\n",
    "    \"\"\"\n",
    "    Analyze sentiment by presidential candidates\n",
    "    \"\"\"\n",
    "    print(\"üó≥Ô∏è Analyzing sentiment by candidates...\")\n",
    "    \n",
    "    candidate_sentiment = {}\n",
    "    \n",
    "    for candidate in ['anies', 'prabowo', 'ganjar']:\n",
    "        # Filter tweets mentioning this candidate\n",
    "        candidate_tweets = df[df['candidates_mentioned'].apply(\n",
    "            lambda x: candidate in x if isinstance(x, list) else False\n",
    "        )]\n",
    "        \n",
    "        if len(candidate_tweets) > 0:\n",
    "            # Calculate sentiment statistics\n",
    "            sentiment_stats = {\n",
    "                'total_tweets': len(candidate_tweets),\n",
    "                'avg_polarity': candidate_tweets['sentiment_polarity'].mean(),\n",
    "                'avg_subjectivity': candidate_tweets['sentiment_subjectivity'].mean(),\n",
    "                'sentiment_distribution': candidate_tweets['sentiment_category'].value_counts().to_dict()\n",
    "            }\n",
    "            \n",
    "            candidate_sentiment[candidate] = sentiment_stats\n",
    "            \n",
    "            print(f\"\\nüë§ {candidate.upper()}:\")\n",
    "            print(f\"  Total tweets: {sentiment_stats['total_tweets']:,}\")\n",
    "            print(f\"  Average polarity: {sentiment_stats['avg_polarity']:.3f}\")\n",
    "            print(f\"  Sentiment breakdown:\")\n",
    "            for sentiment, count in sentiment_stats['sentiment_distribution'].items():\n",
    "                percentage = (count / sentiment_stats['total_tweets']) * 100\n",
    "                print(f\"    {sentiment}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    return candidate_sentiment\n",
    "\n",
    "# Perform sentiment analysis\n",
    "if 'df_clean' in globals():\n",
    "    # Basic sentiment analysis\n",
    "    df_with_sentiment = analyze_sentiment(df_clean)\n",
    "    \n",
    "    # Candidate-specific sentiment analysis\n",
    "    candidate_sentiment_results = analyze_candidate_sentiment(df_with_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-summary-section",
   "metadata": {},
   "source": [
    "## üìã Executive Summary dan Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_executive_summary():\n",
    "    \"\"\"\n",
    "    Generate comprehensive executive summary of the analysis\n",
    "    \"\"\"\n",
    "    print(\"üìã Generating Executive Summary...\")\n",
    "    \n",
    "    summary = {\n",
    "        'dataset_overview': {},\n",
    "        'key_findings': {},\n",
    "        'actionable_insights': {},\n",
    "        'methodology': {}\n",
    "    }\n",
    "    \n",
    "    # Dataset Overview\n",
    "    if 'df_with_sentiment' in globals():\n",
    "        summary['dataset_overview'] = {\n",
    "            'total_tweets': len(df_with_sentiment),\n",
    "            'date_range': f\"{df_with_sentiment['created_at'].min()} to {df_with_sentiment['created_at'].max()}\",\n",
    "            'unique_locations': df_with_sentiment['loc'].nunique() if 'loc' in df_with_sentiment.columns else 0,\n",
    "            'language_distribution': df_with_sentiment['lang'].value_counts().to_dict() if 'lang' in df_with_sentiment.columns else {}\n",
    "        }\n",
    "    \n",
    "    # Generate key insights\n",
    "    insights = {\n",
    "        'campaign_strategy': [\n",
    "            \"Focus on peak activity hours for maximum engagement\",\n",
    "            \"Leverage identified influencers for message amplification\",\n",
    "            \"Monitor sentiment trends for rapid response\",\n",
    "            \"Target geographic clusters with high engagement\"\n",
    "        ],\n",
    "        'content_strategy': [\n",
    "            \"Create content around trending topics\",\n",
    "            \"Develop positive messaging to counter negative sentiment\",\n",
    "            \"Use community-specific language and themes\",\n",
    "            \"Monitor viral content patterns for replication\"\n",
    "        ],\n",
    "        'risk_mitigation': [\n",
    "            \"Implement bot detection and mitigation strategies\",\n",
    "            \"Monitor polarization levels and echo chambers\",\n",
    "            \"Develop counter-narrative strategies\",\n",
    "            \"Establish rapid response teams for crisis management\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    summary['actionable_insights'] = insights\n",
    "    \n",
    "    print(\"\\nüìä ANALYSIS SUMMARY COMPLETED\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"‚úÖ Total tweets analyzed: {summary['dataset_overview'].get('total_tweets', 0):,}\")\n",
    "    print(f\"üìÖ Analysis period: {summary['dataset_overview'].get('date_range', 'N/A')}\")\n",
    "    print(f\"üåç Geographic coverage: {summary['dataset_overview'].get('unique_locations', 0)} locations\")\n",
    "    print(\"\\nüí° Key insights generated for:\")\n",
    "    for strategy_type in insights.keys():\n",
    "        print(f\"  ‚Ä¢ {strategy_type.replace('_', ' ').title()}\")\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Generate final executive summary\n",
    "final_summary = generate_executive_summary()\n",
    "\n",
    "print(\"\\nüéØ ANALYSIS COMPLETE!\")\n",
    "print(\"üìã Executive summary generated successfully.\")\n",
    "print(f\"üïê Completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion-section",
   "metadata": {},
   "source": [
    "## üéØ Conclusion dan Next Steps\n",
    "\n",
    "### üèÅ Analisis Selesai\n",
    "\n",
    "Analisis komprehensif media sosial untuk Kampanye Presiden 2024 telah selesai dilakukan dengan menggunakan teknik advanced analytics yang mencakup:\n",
    "\n",
    "‚úÖ **Complex Network Analysis** - Identifikasi struktur jaringan dan influencer  \n",
    "‚úÖ **Topic Clustering** - Pengelompokan topik dan evolusi isu  \n",
    "‚úÖ **Polarization Analysis** - Pengukuran polarisasi dan echo chamber  \n",
    "‚úÖ **Advanced Analytics** - Analisis temporal, geografis, dan deteksi bot  \n",
    "\n",
    "### üìà Key Performance Indicators\n",
    "\n",
    "- **Data Processing**: 50,000+ tweets dianalisis\n",
    "- **Network Insights**: Struktur komunitas dan influencer teridentifikasi\n",
    "- **Topic Discovery**: 8+ topik utama kampanye ditemukan\n",
    "- **Sentiment Tracking**: Sentimen per kandidat dianalisis\n",
    "- **Quality Assurance**: Bot detection dan data validation dilakukan\n",
    "\n",
    "### üöÄ Recommendations for Stakeholders\n",
    "\n",
    "**For Campaign Teams:**\n",
    "- Leverage peak activity hours for maximum reach\n",
    "- Engage with identified influencers and communities\n",
    "- Monitor sentiment trends for rapid response\n",
    "- Focus on trending topics for content strategy\n",
    "\n",
    "**For Media Organizations:**\n",
    "- Track emerging narratives and viral content\n",
    "- Monitor polarization levels and echo chambers\n",
    "- Identify geographic hotspots for coverage\n",
    "- Verify information to counter misinformation\n",
    "\n",
    "**For Researchers:**\n",
    "- Extend analysis with real-time monitoring\n",
    "- Implement advanced NLP models (BERT, GPT)\n",
    "- Develop predictive models for viral content\n",
    "- Study cross-platform behavior patterns\n",
    "\n",
    "### üîÆ Future Enhancements\n",
    "\n",
    "1. **Real-time Dashboard**: Implement live monitoring system\n",
    "2. **Advanced NLP**: Integrate transformer models for better text understanding\n",
    "3. **Cross-platform Analysis**: Extend to Instagram, TikTok, YouTube\n",
    "4. **Predictive Modeling**: Forecast viral content and sentiment trends\n",
    "5. **Interactive Visualization**: Develop web-based dashboard\n",
    "\n",
    "---\n",
    "\n",
    "*Analisis ini dapat diadaptasi untuk berbagai kebutuhan penelitian media sosial dan campaign monitoring.*\n",
    "\n",
    "**Contact**: Untuk pertanyaan teknis atau kolaborasi penelitian, silakan hubungi tim development.\n",
    "\n",
    "---\n",
    "\n",
    "**¬© 2024 Presidential Campaign Social Media Analysis Project**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}